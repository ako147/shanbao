import os, sqlite3
from typing import List, Tuple
from contextlib import asynccontextmanager

import numpy as np
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModel
import torch

from google import genai
from google.genai import types
from google.genai import errors

# ================== è·¯å¾‘èˆ‡ç’°å¢ƒ ==================
BASE_DIR  = os.path.dirname(os.path.abspath(__file__))
FRONT_DIR = os.path.join(os.path.dirname(BASE_DIR), "frontend")   # ä½ çš„å‰ç«¯è³‡æ–™å¤¾
KB_DIR    = os.path.join(BASE_DIR, "kb_raw")                      # å›ºå®šçŸ¥è­˜åº«
DB_PATH   = os.path.join(BASE_DIR, "embeddings.db")               # SQLite å‘é‡åº«

_embed_model = None
_tokenizer = None

load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    raise RuntimeError("GOOGLE_API_KEY æœªè¨­å®šï¼Œè«‹åœ¨ backend/.env æˆ–ç³»çµ±ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®š")

# === æ¨¡å‹è¨­å®šï¼ˆé›†ä¸­ï¼‰===
EMBED_MODEL   = os.getenv("EMBED_MODEL", "text-embedding-004")
GEN_MODEL     = os.getenv("GEN_MODEL", "gemini-2.0-flash")        # å¯æ”¹ .env åˆ‡æ›
GENAI_VERSION = os.getenv("GENAI_API_VERSION", "v1alpha")              # ä½ çš„ google-genai ç‰ˆæœ¬å°æ‡‰

# ================== è¦å‰‡èˆ‡åˆ†å¡Š ==================
GEM_RULES = """
å›ç­”ä¸€å¾‹ç”¨ç¹é«”ä¸­æ–‡èˆ‡æ¢åˆ—çµæ§‹ã€‚
è‹¥æª¢ç´¢å…§å®¹ä¸è¶³ä»¥æ”¯æŒç­”æ¡ˆï¼Œè«‹æ˜ç¢ºèªªæ˜ã€ŒæŸ¥ç„¡ç›¸é—œè³‡æ–™ã€ï¼Œä¸¦æå‡ºå¯è¡ŒæŸ¥æ‰¾æ–¹å‘ã€‚
è‹¥å•é¡Œèˆ‡æª¢ç´¢å…§å®¹ä¸ç›¸é—œï¼Œè«‹å‘Šè¨´æ°‘çœ¾ã€Œè«‹è¨Šå•èˆ‡å¼µå¸‚é•·ç›¸é—œå•é¡Œã€ï¼Œæåˆ—èˆ‰å¹¾å€‹èˆ‡æª¢ç´¢å…§å®¹æœ‰é—œçš„å•é¡Œä¾›æ°‘çœ¾åƒè€ƒã€‚
ç›¡é‡è§£é‡‹å°ˆæœ‰åè©ï¼Œè®“éå°ˆæ¥­è€…ä¹Ÿèƒ½ç†è§£ã€‚
å¦‚æœ‰å¼•ç”¨æª¢ç´¢ç‰‡æ®µï¼Œè«‹åœ¨å¥å°¾æ¨™ç¤º [1]ã€[2]â€¦ å°æ‡‰æª¢ç´¢æ¸…å–®ã€‚
"""

CHUNK_SIZE = 500
CHUNK_OVERLAP = 120

def chunk_text(text: str, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:
    toks = text.split()
    out = []
    i = 0
    step = max(1, size - overlap)
    while i < len(toks):
        out.append(" ".join(toks[i:i+size]))
        i += step
    return out or [text]

# ================== SQLite å·¥å…· ==================
def init_db():
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS docs (
        id     TEXT PRIMARY KEY,
        doc_id TEXT,
        text   TEXT,
        embed  BLOB
    )
    """)
    cur.execute("""CREATE TABLE IF NOT EXISTS meta (k TEXT PRIMARY KEY, v TEXT)""")
    con.commit()
    con.close()

def add_rows(rows: List[Tuple[str, str, str, np.ndarray]]):
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    for rid, did, text, vec in rows:
        blob = np.asarray(vec, dtype=np.float32).tobytes()
        cur.execute(
            "INSERT OR REPLACE INTO docs(id, doc_id, text, embed) VALUES(?,?,?,?)",
            (rid, did, text, blob),
        )
    con.commit()
    con.close()

def all_embeddings():
    con = sqlite3.connect(DB_PATH)
    cur = con.cursor()
    cur.execute("SELECT id, doc_id, text, embed FROM docs")
    rows = cur.fetchall()
    con.close()
    if not rows:
        # å°‡åœ¨å•Ÿå‹•æ™‚ä»¥ probe è‡ªå‹•åµæ¸¬çœŸæ­£ç¶­åº¦
        return [], [], [], np.zeros((0, 1), dtype=np.float32)
    ids, docids, texts, vecs = [], [], [], []
    for rid, did, txt, blob in rows:
        ids.append(rid); docids.append(did); texts.append(txt)
        vecs.append(np.frombuffer(blob, dtype=np.float32))
    return ids, docids, texts, np.vstack(vecs)

def get_meta(key: str) -> str | None:
    con = sqlite3.connect(DB_PATH); cur = con.cursor()
    cur.execute("SELECT v FROM meta WHERE k=?", (key,))
    row = cur.fetchone()
    con.close()
    return row[0] if row else None

def set_meta(key: str, val: str):
    con = sqlite3.connect(DB_PATH); cur = con.cursor()
    cur.execute("INSERT OR REPLACE INTO meta(k,v) VALUES(?,?)", (key, val))
    con.commit(); con.close()

def cosine_topk(query_vec: np.ndarray, mat: np.ndarray, k: int):
    if mat.shape[0] == 0:
        return []
    q = query_vec / (np.linalg.norm(query_vec) + 1e-9)
    m = mat / (np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9)
    sims = m @ q
    idx = np.argsort(-sims)[:k]
    return idx.tolist()

# ================== Gemini Client & åŒ…è£ ==================
# ä¸€å€‹ client ç”¨é è¨­ç‰ˆæœ¬ï¼ˆå¤šåŠç”¨æ–¼ embeddingsï¼‰
gen = genai.Client(api_key=API_KEY)
# å¦ä¸€å€‹ client å¯æŒ‡å®š API ç‰ˆæœ¬ï¼ˆç”¨æ–¼ç”Ÿæˆï¼›é¿å…ä½ ä¸åŒç’°å¢ƒçš„ç‰ˆæœ¬ä¸ä¸€è‡´ï¼‰
gen_chat = genai.Client(api_key=API_KEY, http_options={'api_version': GENAI_VERSION})

def embed_texts(texts: list[str]) -> list[np.ndarray]:
    """ä¾æ“š EMBED_BACKEND æ±ºå®šç”¨ HuggingFace æˆ– Google GenAI ç”¢ç”Ÿ embedding"""
    backend = os.getenv("EMBED_BACKEND", "google").lower()

    if backend == "huggingface":
        global _embed_model, _tokenizer
        if _embed_model is None:
            print(f"ğŸ”¹ ä½¿ç”¨ HuggingFace æ¨¡å‹ï¼š{os.getenv('EMBED_MODEL')}")
            _tokenizer = AutoTokenizer.from_pretrained(os.getenv("EMBED_MODEL"))
            _embed_model = AutoModel.from_pretrained(os.getenv("EMBED_MODEL"))
        _embed_model.eval()
        with torch.no_grad():
            inputs = _tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
            outputs = _embed_model(**inputs)
            # å–æœ€å¾Œéš±è—å±¤å¹³å‡å€¼
            embeddings = outputs.last_hidden_state.mean(dim=1)
            return [emb.numpy().astype(np.float32) for emb in embeddings]

    # é è¨­ä½¿ç”¨ Google GenAI
    res = gen.models.embed_content(model=os.getenv("EMBED_MODEL", "text-embedding-004"),
                                   contents=texts)
    return [np.array(e.values, dtype=np.float32) for e in res.embeddings]


GEN_MODEL_CANDIDATES = [
    lambda: os.getenv("GEN_MODEL", GEN_MODEL),  # å…ˆç”¨ .env æŒ‡å®šå€¼
    lambda: "gemini-2.0-flash-002",
    lambda: "gemini-1.5-flash",
    lambda: "gemini-1.5-flash-8b",
    lambda: "gemini-1.5-pro",
]

def stream_generate(query: str, sys_inst: str, temperature: float):
    """å˜—è©¦å¤šå€‹å‹è™Ÿï¼Œç¬¬ä¸€å€‹èƒ½ç”¨çš„å°±ä¸²æµè¼¸å‡º"""
    last_err = None
    tried = set()
    for get_name in GEN_MODEL_CANDIDATES:
        name = get_name()
        if not name or name in tried:
            continue
        tried.add(name)
        try:
            stream = gen_chat.models.generate_content_stream(
                model=name,
                config=types.GenerateContentConfig(
                    system_instruction=sys_inst,
                    temperature=temperature,
                ),
                contents=query,  # ç›´æ¥çµ¦å­—ä¸²
            )
            for ev in stream:
                cand = (ev.candidates or [None])[0]
                if cand and cand.content and cand.content.parts:
                    for p in cand.content.parts:
                        if getattr(p, "text", None):
                            yield p.text
            return
        except errors.ClientError as e:
            last_err = e
            continue
    raise last_err if last_err else RuntimeError("No generation model worked.")

# ================== FastAPI ==================
class ChatReq(BaseModel):
    query: str
    top_k: int = 5
    temperature: float = 0.3
    # gen_model: str | None = None  # è‹¥è¦è®“å‰ç«¯è‡¨æ™‚æŒ‡å®šï¼Œå¯åŠ é€™æ¬„ä¸¦èª¿æ•´ä¸Šé¢å€™é¸æ¸…å–®

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ---- å•Ÿå‹•ï¼šå»º DB + æª¢æŸ¥/é‡å»ºç´¢å¼• ----
    init_db()

    # ä»¥æœ€çŸ­æ–‡æœ¬æ¢æ¸¬ç•¶å‰ EMBED_MODEL çš„ç¶­åº¦
    probe_dim = len(embed_texts(["_dim_probe_"])[0])
    db_dim = get_meta("embed_dim")
    if db_dim is None:
        set_meta("embed_dim", str(probe_dim))
    elif int(db_dim) != probe_dim:
        print(f"âš ï¸ åµæ¸¬åˆ°åµŒå…¥ç¶­åº¦æ”¹è®Šï¼šDB={db_dim}, MODEL={probe_dim} â†’ é‡æ–°å»ºç«‹ embeddings.db")
        if os.path.exists(DB_PATH): os.remove(DB_PATH)
        init_db()
        set_meta("embed_dim", str(probe_dim))

    # è‹¥å°šæœªå»ºç«‹ç´¢å¼•ï¼Œå¾ kb_raw è¼‰å…¥ txt/md æª”æ¡ˆåµŒå…¥
    ids, _, _, _ = all_embeddings()
    if not ids:
        os.makedirs(KB_DIR, exist_ok=True)
        files = [f for f in os.listdir(KB_DIR) if f.lower().endswith((".txt", ".md"))]
        if files:
            for fname in files:
                path = os.path.join(KB_DIR, fname)
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read()
                chunks = chunk_text(text)
                vecs = embed_texts(chunks)
                rows = [(f"{fname}-{i}", fname, chunks[i], vecs[i]) for i in range(len(chunks))]
                add_rows(rows)
                print(f"å·²å»ºç«‹çŸ¥è­˜åº«ï¼š{fname}ï¼ˆ{len(chunks)} åˆ†å¡Šï¼‰")
            print("âœ… å›ºå®šçŸ¥è­˜åº«è¼‰å…¥å®Œæˆã€‚")
        else:
            print("âš ï¸ kb_raw å…§æ²’æœ‰ .txt/.md æª”æ¡ˆï¼ŒçŸ¥è­˜åº«ç‚ºç©ºã€‚")
    else:
        print("å·²å­˜åœ¨ embedding çŸ¥è­˜åº«ï¼Œç•¥éé‡å»ºã€‚")

    yield  # ---- æœå‹™é–‹å§‹ ----

    # ---- é—œé–‰ï¼šå¯åœ¨æ­¤é‡‹æ”¾è³‡æº ----
    # print("æœå‹™é—œé–‰")

app = FastAPI(title="Gemini RAG (Fixed KB, NumPy/SQLite)", lifespan=lifespan)

# CORSï¼ˆåŒç¶²åŸŸå³å¯ï¼›è‹¥è·¨ç¶²åŸŸå†é™ç¸® allow_originsï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ›è¼‰æ•´å€‹å‰ç«¯è³‡æ–™å¤¾åˆ° /assets
if os.path.isdir(FRONT_DIR):
    app.mount("/assets", StaticFiles(directory=FRONT_DIR), name="assets")

# é¦–é ï¼šå›å‚³ people.html
@app.get("/")
def serve_people():
    return FileResponse(os.path.join(FRONT_DIR, "people.html"))

# èŠå¤©ï¼ˆä¸²æµï¼‰
@app.post("/chat/stream")
async def chat_stream(req: ChatReq):
    # 1) å–çŸ©é™£
    _, _, texts, mat = all_embeddings()

    # 2) æŸ¥è©¢å‘é‡
    q_vec = embed_texts([req.query])[0]

    # 3) Top-k ç‰‡æ®µ
    idxs = cosine_topk(q_vec, mat, req.top_k)
    contexts = [texts[i] for i in idxs]
    context_block = "\n\n".join(f"[{i+1}] {c}" for i, c in enumerate(contexts)) if contexts else "ï¼ˆç„¡æª¢ç´¢ç‰‡æ®µï¼‰"

    # 4) system instruction
    sys_inst = (
        GEM_RULES
        + "\n\nä»¥ä¸‹æ˜¯æª¢ç´¢åˆ°çš„ç‰‡æ®µï¼Œå¿…è¦æ™‚å¯å¼•ç”¨ï¼š\n"
        + "=== æª¢ç´¢ç‰‡æ®µ ===\n"
        + context_block
    )

    # 5) ä¸²æµå›å‚³
    def stream_gen():
        yield from stream_generate(req.query, sys_inst, req.temperature)
    return StreamingResponse(stream_gen(), media_type="text/plain")

# æ¸…åº«ï¼ˆé‡å»ºç”¨ï¼‰
@app.post("/reset")
async def reset():
    try:
        if os.path.exists(DB_PATH):
            os.remove(DB_PATH)
    except Exception:
        pass
    init_db()
    return {"ok": True}